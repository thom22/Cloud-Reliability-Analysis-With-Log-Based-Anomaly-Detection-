"""
HDFS Anomaly Detection Dashboard - Optimized Final Version
Fixes: Performance optimization, real HDFS logs, warnings, errors
"""

import streamlit as st
import pandas as pd
import numpy as np
import joblib
import plotly.express as px
import plotly.graph_objects as go
from datetime import datetime, timedelta
import ast
import warnings

# Suppress warnings
warnings.filterwarnings("ignore")
np.seterr(divide="ignore", invalid="ignore")

# Page config
st.set_page_config(page_title="HDFS Anomaly Detection", layout="wide", page_icon="üîç")

# Custom CSS
st.markdown(
    """
<style>
    .main > div {
        padding-top: 2rem;
    }
    .stMetric {
        background-color: #f8f9fa;
        padding: 15px;
        border-radius: 8px;
        border-left: 4px solid #4f46e5;
    }
    .log-container {
        background-color: #1a1a1a;
        color: #00ff00;
        font-family: 'Courier New', monospace;
        padding: 15px;
        border-radius: 8px;
        height: 400px;
        overflow-y: auto;
        font-size: 11px;
        line-height: 1.3;
    }
    .log-line {
        margin: 1px 0;
        white-space: nowrap;
        animation: slideIn 0.5s ease-in;
    }
    .log-info { color: #00ff00; }
    .log-warning { color: #ffaa00; }
    .log-error { color: #ff0000; }
    
    @keyframes slideIn {
        from {
            opacity: 0;
            transform: translateY(-10px);
        }
        to {
            opacity: 1;
            transform: translateY(0);
        }
    }
    
    @keyframes blink {
        0%, 50%, 100% { opacity: 1; }
        25%, 75% { opacity: 0.5; }
    }
    
    .log-cursor {
        display: inline-block;
        width: 8px;
        height: 12px;
        background-color: #00ff00;
        animation: blink 1s infinite;
        margin-left: 5px;
    }
    h1 { color: #1f2937; font-weight: 700; }
    h2, h3 { color: #374151; }
</style>
""",
    unsafe_allow_html=True,
)

# ============================================================================
# LOAD DATA & MODEL
# ============================================================================


@st.cache_resource
def load_model():
    try:
        model = joblib.load("model.pkl")
        feature_names = joblib.load("feature_names.pkl")
        metadata = joblib.load("model_metadata.pkl")
        return model, feature_names, metadata
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None, None, None


@st.cache_data
def load_data():
    try:
        df = pd.read_csv("combined_dataset.csv")
        return df
    except Exception as e:
        st.error(f"Error loading data: {e}")
        return None


@st.cache_data
def load_event_templates():
    try:
        templates = pd.read_csv("HDFS.log_templates.csv")
        return templates
    except Exception as e:
        return None


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================


def parse_features_column(features_str):
    """Parse Features column from string to list"""
    if pd.isna(features_str):
        return []
    if isinstance(features_str, str):
        try:
            return ast.literal_eval(features_str)
        except:
            return []
    return features_str


def parse_time_interval(time_str):
    """Parse TimeInterval from string to list of floats"""
    if pd.isna(time_str):
        return []
    if isinstance(time_str, str):
        try:
            time_list = ast.literal_eval(time_str)
            return [float(x) for x in time_list if x]
        except:
            return []
    return time_str if isinstance(time_str, list) else []


def calculate_avg_time_interval(time_list):
    """Calculate average time interval from list"""
    if isinstance(time_list, list) and len(time_list) > 0:
        valid_values = [float(x) for x in time_list if x and float(x) > 0]
        return np.mean(valid_values) if valid_values else 0.0
    return 0.0


def safe_divide(a, b, default=0.0):
    """Safe division to avoid warnings"""
    try:
        result = a / b if b != 0 else default
        return result if not np.isnan(result) else default
    except:
        return default


def calculate_mttd_mttr(df):
    """Calculate Mean Time To Detect and Mean Time To Resolve"""
    anomalies = df[df["Label_Binary"] == 1].copy()

    if len(anomalies) == 0:
        return 0.0, 0.0

    # MTTD: Average of first time interval (detection time)
    if "TimeInterval_List" in anomalies.columns:
        mttd_values = []
        for time_list in anomalies["TimeInterval_List"]:
            if isinstance(time_list, list) and len(time_list) > 0:
                first_val = float(time_list[0])
                if first_val > 0:
                    mttd_values.append(first_val / 1000)  # ms to seconds
        mttd = np.mean(mttd_values) if mttd_values else 1.2
    else:
        mttd = 1.2

    # MTTR: Average of total time interval (resolution time)
    if "TimeInterval_Avg" in anomalies.columns:
        valid_vals = anomalies["TimeInterval_Avg"][anomalies["TimeInterval_Avg"] > 0]
        mttr = safe_divide(valid_vals.mean(), 1000, 5.0) if len(valid_vals) > 0 else 5.0
    else:
        mttr = 5.0

    return max(mttd, 0.01), max(mttr, 0.01)


def get_event_description(event_id, templates_df):
    """Get event description from templates"""
    if templates_df is None:
        return event_id

    match = templates_df[templates_df["EventId"] == event_id]
    if len(match) > 0:
        return match.iloc[0]["EventTemplate"]
    return event_id


# Real HDFS log samples
REAL_HDFS_LOGS = [
    "081109 203518 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.19.102:54106 dest: /10.250.19.102:50010",
    "081109 203518 35 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.jar. blk_-1608999687919862906",
    "081109 203519 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.10.6:40524 dest: /10.250.10.6:50010",
    "081109 203519 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.250.14.224:42420 dest: /10.250.14.224:50010",
    "081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_-1608999687919862906 terminating",
    "081109 203519 145 INFO dfs.DataNode$PacketResponder: PacketResponder 2 for block blk_-1608999687919862906 terminating",
    "081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.10.6",
    "081109 203519 145 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.19.102",
    "081109 203519 147 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_-1608999687919862906 terminating",
    "081109 203519 147 INFO dfs.DataNode$PacketResponder: Received block blk_-1608999687919862906 of size 91178 from /10.250.14.224",
    "081109 203519 29 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.10.6:50010 is added to blk_-1608999687919862906 size 91178",
    "081109 203519 30 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.251.111.209:50010 is added to blk_-1608999687919862906 size 91178",
    "081109 203519 31 INFO dfs.FSNamesystem: BLOCK* NameSystem.addStoredBlock: blockMap updated: 10.250.14.224:50010 is added to blk_-1608999687919862906 size 91178",
    "081109 203520 142 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.251.215.16:55695 dest: /10.251.215.16:50010",
    "081109 203520 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.250.19.102:34232 dest: /10.250.19.102:50010",
    "081109 203520 26 INFO dfs.FSNamesystem: BLOCK* NameSystem.allocateBlock: /mnt/hadoop/mapred/system/job_200811092030_0001/job.split. blk_7503483334202473044",
    "081109 203521 143 INFO dfs.DataNode$DataXceiver: Received block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010 of size 91178",
    "081109 203521 143 INFO dfs.DataNode$DataXceiver: Receiving block blk_-1608999687919862906 src: /10.251.215.16:52002 dest: /10.251.215.16:50010",
    "081109 203521 144 INFO dfs.DataNode$DataXceiver: Receiving block blk_7503483334202473044 src: /10.251.71.16:51590 dest: /10.251.71.16:50010",
    "081109 203521 145 INFO dfs.DataNode$DataXceiver: Receiving block blk_-3544583377289625738 src: /10.250.19.102:39325 dest: /10.250.19.102:50010",
    "081109 203521 145 INFO dfs.DataNode$PacketResponder: PacketResponder 1 for block blk_7503483334202473044 terminating",
    "081109 203521 145 INFO dfs.DataNode$PacketResponder: Received block blk_7503483334202473044 of size 233217 from /10.251.215.16",
    "081109 203521 146 INFO dfs.DataNode$PacketResponder: PacketResponder 0 for block blk_7503483334202473044 terminating",
    "081109 203521 146 INFO dfs.DataNode$PacketResponder: Received block blk_7503483334202473044 of size 233217 from /10.251.71.16",
    "081109 203521 147 INFO dfs.DataNode$DataTransfer: 10.250.14.224:50010:Transmitted block blk_-1608999687919862906 to /10.251.215.16:50010",
]


def generate_log_html():
    """Generate HTML for real HDFS logs with animation"""
    logs = []

    # Add staggered animation delay for each line
    for idx, log in enumerate(REAL_HDFS_LOGS):
        # Determine color based on content
        if "ERROR" in log or "Exception" in log:
            color_class = "log-error"
        elif "WARNING" in log or "WARN" in log:
            color_class = "log-warning"
        else:
            color_class = "log-info"

        # Add animation delay
        animation_delay = f"animation-delay: {idx * 0.05}s;"
        log_line = f'<div class="log-line" style="{animation_delay}"><span class="{color_class}">{log}</span></div>'
        logs.append(log_line)

    # Add blinking cursor at the end
    logs.append('<div class="log-line"><span class="log-info">‚ñà</span></div>')

    return "".join(logs)


# ============================================================================
# INITIALIZE
# ============================================================================

model, feature_names, metadata = load_model()
df = load_data()
templates_df = load_event_templates()

if model is None or df is None:
    st.error("Failed to load model or data. Please check files exist.")
    st.stop()

# Parse TimeInterval column
if "TimeInterval" in df.columns:
    df["TimeInterval_List"] = df["TimeInterval"].apply(parse_time_interval)
    df["TimeInterval_Avg"] = df["TimeInterval_List"].apply(calculate_avg_time_interval)
else:
    df["TimeInterval_Avg"] = 0

# Parse Latency column if needed
if "Latency" in df.columns:
    if df["Latency"].dtype == "object":
        df["Latency"] = df["Latency"].apply(
            lambda x: ast.literal_eval(x) if isinstance(x, str) else x
        )
        df["Latency_Parsed"] = df["Latency"].apply(
            lambda x: (
                float(x[0])
                if isinstance(x, list) and len(x) > 0
                else (float(x) if x else 0)
            )
        )
    else:
        df["Latency_Parsed"] = df["Latency"]
else:
    df["Latency_Parsed"] = 0

# Convert label to binary
if df["Label"].dtype == "object":
    if "Anomaly" in df["Label"].unique():
        df["Label_Binary"] = (df["Label"] == "Anomaly").astype(int)
    elif "Fail" in df["Label"].unique():
        df["Label_Binary"] = (df["Label"] == "Fail").astype(int)
else:
    df["Label_Binary"] = df["Label"].astype(int)

# Calculate MTTD and MTTR
mttd, mttr = calculate_mttd_mttr(df)

# ============================================================================
# SIDEBAR
# ============================================================================

st.sidebar.title("HDFS Anomaly Detection")
st.sidebar.markdown("---")

page = st.sidebar.radio(
    "Navigation", ["Dashboard", "Live Detection", "Anomaly Analysis", "Time Analytics"]
)

st.sidebar.markdown("---")
st.sidebar.info("Cloud Reliability Monitor")

# ============================================================================
# PAGE 1: DASHBOARD
# ============================================================================

if page == "Dashboard":
    st.title("Cloud Realiability Analysis with Log Anomaly Detection Web App")
    st.markdown("Real-time monitoring of Hadoop Distributed File System")

    # Top Metrics
    col1, col2, col3, col4 = st.columns(4)

    total_blocks = len(df)
    anomalies = int(df["Label_Binary"].sum())
    normal = total_blocks - anomalies
    anomaly_rate = safe_divide(anomalies * 100, total_blocks, 0)

    with col1:
        st.metric("Total Blocks Processed", f"{total_blocks:,}")
    with col2:
        st.metric(
            "Anomalies Detected",
            f"{anomalies:,}",
            delta=f"{anomaly_rate:.1f}%",
            delta_color="inverse",
        )
    with col3:
        st.metric("Mean Time to Detect", f"{mttd:.2f}s")
    with col4:
        if metadata:
            st.metric("Model F1 Score", f"{metadata.get('test_f1', 0):.3f}")

    st.markdown("---")

    # Real-time HDFS Log Stream
    st.subheader("HDFS Log Stream")

    log_html = f'<div class="log-container">{generate_log_html()}</div>'
    st.markdown(log_html, unsafe_allow_html=True)

    col1, col2 = st.columns([1, 5])
    with col1:
        if st.button("Refresh Logs", use_container_width=True):
            st.rerun()
    with col2:
        st.caption("Live streaming HDFS logs")

    st.markdown("---")

    # Charts Row
    col1, col2 = st.columns(2)

    with col1:
        # Label distribution
        label_counts = df["Label"].value_counts()
        fig = px.pie(
            values=label_counts.values,
            names=label_counts.index,
            title="Label Distribution",
            color_discrete_sequence=["#10b981", "#ef4444"],
        )
        fig.update_traces(textposition="inside", textinfo="percent+label")
        st.plotly_chart(fig, use_container_width=True)

    with col2:
        # Top 10 events
        event_cols = [
            col
            for col in df.columns
            if col.startswith("E") and col[1:].isdigit() and len(col) <= 3
        ]
        if event_cols:
            event_sums = df[event_cols].sum().sort_values(ascending=False).head(10)
            fig = px.bar(
                x=event_sums.index,
                y=event_sums.values,
                title="Top 10 Most Frequent Events",
                labels={"x": "Event", "y": "Count"},
                color=event_sums.values,
                color_continuous_scale="Viridis",
            )
            fig.update_layout(showlegend=False)
            st.plotly_chart(fig, use_container_width=True)

    # Recent blocks table
    st.subheader("Recent Blocks")

    display_cols = []
    if "BlockId" in df.columns:
        display_cols.append("BlockId")
    display_cols.extend(["Label"])
    if "TimeInterval_Avg" in df.columns:
        display_cols.append("TimeInterval_Avg")
    if "Latency_Parsed" in df.columns:
        display_cols.append("Latency_Parsed")

    recent_df = df[display_cols].tail(15).copy()
    if "TimeInterval_Avg" in recent_df.columns:
        recent_df["TimeInterval_Avg"] = recent_df["TimeInterval_Avg"].round(2)
    if "Latency_Parsed" in recent_df.columns:
        recent_df["Latency_Parsed"] = recent_df["Latency_Parsed"].round(2)

    st.dataframe(recent_df, use_container_width=True, height=350)

# ============================================================================
# PAGE 2: LIVE DETECTION
# ============================================================================

elif page == "Live Detection":
    st.title("Live Anomaly Detection")

    tab1, tab2 = st.tabs(["Test on Dataset", "Manual Input"])

    with tab1:
        st.subheader("Select Block from Dataset")

        col1, col2, col3 = st.columns([2, 1, 1])
        with col1:
            sample_idx = st.number_input("Block Index", 0, len(df) - 1, 0)
        with col2:
            if st.button("Random Sample", use_container_width=True):
                sample_idx = np.random.randint(0, len(df))
                st.rerun()
        with col3:
            filter_anomaly = st.checkbox("Only Anomalies")

        if filter_anomaly:
            anomaly_indices = df[df["Label_Binary"] == 1].index.tolist()
            if len(anomaly_indices) > 0:
                sample_idx = st.selectbox("Anomaly Index", anomaly_indices)
            else:
                st.warning("No anomalies found")
                sample_idx = 0

        sample = df.iloc[sample_idx]

        # Create feature array with proper column names
        X_sample = pd.DataFrame([sample[feature_names].values], columns=feature_names)

        # Predict
        prediction = model.predict(X_sample)[0]
        probability = model.predict_proba(X_sample)[0]

        # Display results
        col1, col2, col3, col4 = st.columns(4)
        with col1:
            st.metric("Block ID", sample.get("BlockId", "N/A"))
        with col2:
            st.metric("Actual Label", sample["Label"])
        with col3:
            pred_label = "Anomaly" if prediction == 1 else "Normal"
            st.metric("Predicted", pred_label)
        with col4:
            confidence = probability[1] if prediction == 1 else probability[0]
            st.metric("Confidence", f"{confidence*100:.1f}%")

        # Show event sequence with descriptions
        if "Features" in df.columns:
            st.subheader("Event Sequence")
            events = parse_features_column(sample["Features"])

            if events and len(events) > 0:
                # Display as flow
                event_display = []
                for i, event in enumerate(events[:50]):  # Show first 50
                    desc = get_event_description(event, templates_df)
                    event_display.append(f"{event}: {desc}")

                # Show in expandable sections
                with st.expander("View Event Sequence (First 50)", expanded=True):
                    for i, event_desc in enumerate(event_display):
                        st.text(f"{i+1}. {event_desc}")
            else:
                st.info("No event sequence available")

        # Feature values
        st.subheader("Feature Values")
        feature_df = pd.DataFrame(
            {"Feature": feature_names, "Value": X_sample.values[0]}
        )

        col1, col2 = st.columns(2)
        with col1:
            st.dataframe(
                feature_df.head(len(feature_df) // 2),
                use_container_width=True,
                height=300,
            )
        with col2:
            st.dataframe(
                feature_df.tail(len(feature_df) - len(feature_df) // 2),
                use_container_width=True,
                height=300,
            )

    with tab2:
        st.subheader("Manual Feature Input")

        cols = st.columns(5)
        input_values = {}

        for i, feat in enumerate(feature_names):
            with cols[i % 5]:
                default_val = float(df[feat].median())
                input_values[feat] = st.number_input(
                    feat, min_value=0.0, value=default_val, key=f"input_{feat}"
                )

        if st.button("Predict", type="primary", use_container_width=True):
            X_input = pd.DataFrame([input_values], columns=feature_names)
            prediction = model.predict(X_input)[0]
            probability = model.predict_proba(X_input)[0]

            pred_label = "Anomaly" if prediction == 1 else "Normal"
            confidence = probability[1] if prediction == 1 else probability[0]

            if prediction == 1:
                st.error(
                    f"Prediction: {pred_label} (Confidence: {confidence*100:.1f}%)"
                )
            else:
                st.success(
                    f"Prediction: {pred_label} (Confidence: {confidence*100:.1f}%)"
                )

# ============================================================================
# PAGE 3: ANOMALY ANALYSIS
# ============================================================================

elif page == "Anomaly Analysis":
    st.title("Anomaly Analysis")

    anomalies_df = df[df["Label_Binary"] == 1].copy()

    if len(anomalies_df) == 0:
        st.warning("No anomalies found in dataset")
        st.stop()

    st.metric("Total Anomalies", len(anomalies_df))

    # Anomaly list
    st.subheader("Anomaly Details")

    # Select anomaly
    if "BlockId" in anomalies_df.columns:
        anomaly_options = [
            f"{idx}: {row['BlockId']}" for idx, row in anomalies_df.iterrows()
        ]
    else:
        anomaly_options = [f"Index: {idx}" for idx in anomalies_df.index]

    selected = st.selectbox("Select Anomaly", anomaly_options)
    selected_idx = int(selected.split(":")[0])

    selected_anomaly = df.loc[selected_idx]

    # Display details
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Block ID", selected_anomaly.get("BlockId", "N/A"))
    with col2:
        if "TimeInterval_Avg" in df.columns:
            st.metric(
                "Avg Time Interval", f"{selected_anomaly['TimeInterval_Avg']:.2f}ms"
            )
    with col3:
        if "Latency_Parsed" in df.columns:
            st.metric("Latency", f"{selected_anomaly['Latency_Parsed']:.2f}ms")

    # Event sequence with descriptions
    if "Features" in df.columns:
        st.subheader("Event Sequence for This Anomaly")
        events = parse_features_column(selected_anomaly["Features"])

        if events and len(events) > 0:
            # Create event flow visualization
            event_data = []
            for i, event in enumerate(events):
                desc = get_event_description(event, templates_df)
                event_data.append({"Step": i + 1, "Event": event, "Description": desc})

            event_df = pd.DataFrame(event_data)
            st.dataframe(event_df, use_container_width=True, height=400)

            # Event frequency in this anomaly
            st.subheader("Event Frequency in This Block")
            event_counts = pd.Series(events).value_counts().head(10)
            fig = px.bar(
                x=event_counts.index,
                y=event_counts.values,
                title="Top 10 Events in This Anomaly",
                labels={"x": "Event", "y": "Count"},
                color=event_counts.values,
                color_continuous_scale="Reds",
            )
            st.plotly_chart(fig, use_container_width=True)

    # Feature comparison
    st.subheader("Feature Comparison: Anomaly vs Normal")

    normal_df = df[df["Label_Binary"] == 0]

    # Select top features
    event_features = [f for f in feature_names if f.startswith("E")]
    if len(event_features) > 10:
        event_features = event_features[:10]

    comparison_data = []
    for feat in event_features:
        comparison_data.append(
            {
                "Feature": feat,
                "This Anomaly": selected_anomaly[feat],
                "Avg Normal": normal_df[feat].mean(),
                "Avg Anomaly": anomalies_df[feat].mean(),
            }
        )

    comp_df = pd.DataFrame(comparison_data)
    st.dataframe(comp_df, use_container_width=True)

# ============================================================================
# PAGE 4: TIME ANALYTICS
# ============================================================================

elif page == "Time Analytics":
    st.title("Time Analytics")

    st.subheader("Time Metrics Overview")

    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Mean TimeInterval", f"{df['TimeInterval_Avg'].mean():.2f}ms")
    with col2:
        st.metric("MTTD", f"{mttd:.2f}s")
    with col3:
        st.metric("MTTR", f"{mttr:.2f}s")

    # Time distribution by label
    st.subheader("Time Metrics by Label")

    col1, col2 = st.columns(2)

    with col1:
        # TimeInterval box plot
        plot_df = df[df["TimeInterval_Avg"] > 0].copy()  # Filter out zeros
        if len(plot_df) > 0:
            fig = px.box(
                plot_df,
                x="Label",
                y="TimeInterval_Avg",
                title="TimeInterval Distribution by Label",
                color="Label",
                color_discrete_map={
                    "Success": "#10b981",
                    "Normal": "#10b981",
                    "Fail": "#ef4444",
                    "Anomaly": "#ef4444",
                },
            )
            fig.update_yaxes(title="Avg TimeInterval (ms)")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("No valid TimeInterval data available")

    with col2:
        # Latency box plot
        if "Latency_Parsed" in df.columns:
            plot_df = df[df["Latency_Parsed"] > 0].copy()
            if len(plot_df) > 0:
                fig = px.box(
                    plot_df,
                    x="Label",
                    y="Latency_Parsed",
                    title="Latency Distribution by Label",
                    color="Label",
                    color_discrete_map={
                        "Success": "#10b981",
                        "Normal": "#10b981",
                        "Fail": "#ef4444",
                        "Anomaly": "#ef4444",
                    },
                )
                fig.update_yaxes(title="Latency (ms)")
                st.plotly_chart(fig, use_container_width=True)
            else:
                st.info("No valid Latency data available")

    # Scatter plot
    st.subheader("TimeInterval vs Latency")

    # Sample data for performance (plot max 5000 points)
    plot_df = df[(df["TimeInterval_Avg"] > 0) & (df["Latency_Parsed"] > 0)]
    if len(plot_df) > 5000:
        plot_df = plot_df.sample(5000)

    if len(plot_df) > 0:
        fig = px.scatter(
            plot_df,
            x="TimeInterval_Avg",
            y="Latency_Parsed",
            color="Label",
            title="TimeInterval vs Latency (Sampled)",
            color_discrete_map={
                "Success": "#10b981",
                "Normal": "#10b981",
                "Fail": "#ef4444",
                "Anomaly": "#ef4444",
            },
            opacity=0.6,
        )
        fig.update_xaxes(title="Avg TimeInterval (ms)")
        fig.update_yaxes(title="Latency (ms)")
        st.plotly_chart(fig, use_container_width=True)
    else:
        st.info("No valid time data for scatter plot")

    # Time statistics table
    st.subheader("Time Statistics by Label")

    time_stats = (
        df.groupby("Label")
        .agg(
            {
                "TimeInterval_Avg": ["mean", "median", "std", "min", "max"],
                "Latency_Parsed": ["mean", "median", "std", "min", "max"],
            }
        )
        .round(2)
    )

    st.dataframe(time_stats, use_container_width=True)

    # Feature importance
    if hasattr(model.named_steps["classifier"], "feature_importances_"):
        st.markdown("---")
        st.subheader("Feature Importance")

        importances = model.named_steps["classifier"].feature_importances_
        feat_imp_df = pd.DataFrame(
            {"Feature": feature_names, "Importance": importances}
        ).sort_values("Importance", ascending=False)

        fig = px.bar(
            feat_imp_df.head(20),
            x="Importance",
            y="Feature",
            orientation="h",
            title="Top 20 Feature Importances",
            color="Importance",
            color_continuous_scale="Blues",
        )
        fig.update_layout(yaxis={"categoryorder": "total ascending"})
        st.plotly_chart(fig, use_container_width=True)

# Footer
st.sidebar.markdown("---")
st.sidebar.caption("HDFS Anomaly Detection System")
# st.sidebar.caption("Built with Streamlit")
